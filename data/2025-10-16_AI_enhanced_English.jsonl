{"id": "2510.12901", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12901", "abs": "https://arxiv.org/abs/2510.12901", "authors": ["Haithem Turki", "Qi Wu", "Xin Kang", "Janick Martinez Esturo", "Shengyu Huang", "Ruilong Li", "Zan Gojcic", "Riccardo de Lutio"], "title": "SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms", "comment": "Project page: https://research.nvidia.com/labs/sil/projects/simuli", "summary": "Rigorous testing of autonomous robots, such as self-driving vehicles, is\nessential to ensure their safety in real-world deployments. This requires\nbuilding high-fidelity simulators to test scenarios beyond those that can be\nsafely or exhaustively collected in the real-world. Existing neural rendering\nmethods based on NeRF and 3DGS hold promise but suffer from low rendering\nspeeds or can only render pinhole camera models, hindering their suitability to\napplications that commonly require high-distortion lenses and LiDAR data.\nMulti-sensor simulation poses additional challenges as existing methods handle\ncross-sensor inconsistencies by favoring the quality of one modality at the\nexpense of others. To overcome these limitations, we propose SimULi, the first\nmethod capable of rendering arbitrary camera models and LiDAR data in\nreal-time. Our method extends 3DGUT, which natively supports complex camera\nmodels, with LiDAR support, via an automated tiling strategy for arbitrary\nspinning LiDAR models and ray-based culling. To address cross-sensor\ninconsistencies, we design a factorized 3D Gaussian representation and\nanchoring strategy that reduces mean camera and depth error by up to 40%\ncompared to existing methods. SimULi renders 10-20x faster than ray tracing\napproaches and 1.5-10x faster than prior rasterization-based work (and handles\na wider range of camera models). When evaluated on two widely benchmarked\nautonomous driving datasets, SimULi matches or exceeds the fidelity of existing\nstate-of-the-art methods across numerous camera and LiDAR metrics.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13714", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13714", "abs": "https://arxiv.org/abs/2510.13714", "authors": ["Dan Jacobellis", "Mateen Ulhaq", "Fabien Racap\u00e9", "Hyomin Choi", "Neeraja J. Yadwadkar"], "title": "Dedelayed: Deleting remote inference delay via on-device correction", "comment": null, "summary": "Remote inference allows lightweight devices to leverage powerful cloud\nmodels. However, communication network latency makes predictions stale and\nunsuitable for real-time tasks. To address this, we introduce Dedelayed, a\ndelay-corrective method that mitigates arbitrary remote inference delays,\nallowing the local device to produce low-latency outputs in real time. Our\nmethod employs a lightweight local model that processes the current frame and\nfuses in features that a heavyweight remote model computes from past frames. On\nvideo from the BDD100K driving dataset, Dedelayed improves semantic\nsegmentation accuracy over the stronger of the local-only and remote-only\nbaselines across all realistic communication network delays beyond 33 ms.\nWithout incurring additional delay, it improves accuracy by 6.4 mIoU compared\nto fully local inference and 9.8 mIoU compared to remote inference, for a\nround-trip delay of 100 ms. The advantage grows under longer delays and\nhigher-motion scenes, as delay-mitigated split inference sustains accuracy more\neffectively, providing clear advantages for real-time tasks that must remain\naligned with the current world state.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13037", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13037", "abs": "https://arxiv.org/abs/2510.13037", "authors": ["Tianmin Xie", "Yanfei Zhou", "Ziyi Liang", "Stefano Favaro", "Matteo Sesia"], "title": "Conformal Inference for Open-Set and Imbalanced Classification", "comment": null, "summary": "This paper presents a conformal prediction method for classification in\nhighly imbalanced and open-set settings, where there are many possible classes\nand not all may be represented in the data. Existing approaches require a\nfinite, known label space and typically involve random sample splitting, which\nworks well when there is a sufficient number of observations from each class.\nConsequently, they have two limitations: (i) they fail to provide adequate\ncoverage when encountering new labels at test time, and (ii) they may become\noverly conservative when predicting previously seen labels. To obtain valid\nprediction sets in the presence of unseen labels, we compute and integrate into\nour predictions a new family of conformal p-values that can test whether a new\ndata point belongs to a previously unseen class. We study these p-values\ntheoretically, establishing their optimality, and uncover an intriguing\nconnection with the classical Good--Turing estimator for the probability of\nobserving a new species. To make more efficient use of imbalanced data, we also\ndevelop a selective sample splitting algorithm that partitions training and\ncalibration data based on label frequency, leading to more informative\npredictions. Despite breaking exchangeability, this allows maintaining\nfinite-sample guarantees through suitable re-weighting. With both simulated and\nreal data, we demonstrate our method leads to prediction sets with valid\ncoverage even in challenging open-set scenarios with infinite numbers of\npossible labels, and produces more informative predictions under extreme class\nimbalance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.12927", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.12927", "abs": "https://arxiv.org/abs/2510.12927", "authors": ["Haolin Li", "Hoda Bidkhori"], "title": "FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment", "comment": null, "summary": "We introduce a novel framework for Federated Class Incremental Learning,\ncalled Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA is\ndesigned to capture task-specific knowledge and model uncertainty in a scalable\nand communication-efficient manner. At the client side, the\nCardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed task\nembeddings that encode task knowledge, address statistical heterogeneity, and\nquantify data uncertainty. Importantly, CATE maintains a fixed parameter size\nregardless of the number of tasks, which ensures scalability across long task\nsequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance to\nmeasure inter-task gaps between Gaussian embeddings. We formulate the\nWasserstein loss to enforce inter-task separation. This probabilistic\nformulation not only enhances representation learning but also preserves\ntask-level privacy by avoiding the direct transmission of latent embeddings,\naligning with the privacy constraints in federated learning. Extensive\nempirical evaluations on popular datasets demonstrate that FedGTEA achieves\nsuperior classification performance and significantly mitigates forgetting,\nconsistently outperforming strong existing baselines.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13094", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13094", "abs": "https://arxiv.org/abs/2510.13094", "authors": ["Aaradhya Pandey", "Arnab Auddy", "Haolin Zou", "Arian Maleki", "Sanjeev Kulkarni"], "title": "Gaussian Certified Unlearning in High Dimensions: A Hypothesis Testing Approach", "comment": "Comments welcome!", "summary": "Machine unlearning seeks to efficiently remove the influence of selected data\nwhile preserving generalization. Significant progress has been made in low\ndimensions $(p \\ll n)$, but high dimensions pose serious theoretical challenges\nas standard optimization assumptions of $\\Omega(1)$ strong convexity and $O(1)$\nsmoothness of the per-example loss $f$ rarely hold simultaneously in\nproportional regimes $(p\\sim n)$. In this work, we introduce\n$\\varepsilon$-Gaussian certifiability, a canonical and robust notion\nwell-suited to high-dimensional regimes, that optimally captures a broad class\nof noise adding mechanisms. Then we theoretically analyze the performance of a\nwidely used unlearning algorithm based on one step of the Newton method in the\nhigh-dimensional setting described above. Our analysis shows that a single\nNewton step, followed by a well-calibrated Gaussian noise, is sufficient to\nachieve both privacy and accuracy in this setting. This result stands in sharp\ncontrast to the only prior work that analyzes machine unlearning in high\ndimensions \\citet{zou2025certified}, which relaxes some of the standard\noptimization assumptions for high-dimensional applicability, but operates under\nthe notion of $\\varepsilon$-certifiability. That work concludes %that a single\nNewton step is insufficient even for removing a single data point, and that at\nleast two steps are required to ensure both privacy and accuracy. Our result\nleads us to conclude that the discrepancy in the number of steps arises because\nof the sub optimality of the notion of $\\varepsilon$-certifiability and its\nincompatibility with noise adding mechanisms, which $\\varepsilon$-Gaussian\ncertifiability is able to overcome optimally.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.12946", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.12946", "abs": "https://arxiv.org/abs/2510.12946", "authors": ["Daniel C. Qi", "Kenshiro Oguri", "Puneet Singla", "Maruthi R. Akella"], "title": "Non-Gaussian Distribution Steering in Nonlinear Dynamics with Conjugate Unscented Transformation", "comment": null, "summary": "In highly nonlinear systems such as the ones commonly found in astrodynamics,\nGaussian distributions generally evolve into non-Gaussian distributions. This\npaper introduces a method for effectively controlling non-Gaussian\ndistributions in nonlinear environments using optimized linear feedback\ncontrol. This paper utilizes Conjugate Unscented Transformation to quantify the\nhigher-order statistical moments of non-Gaussian distributions. The formulation\nfocuses on controlling and constraining the sigma points associated with the\nuncertainty quantification, which would thereby reflect the control of the\nentire distribution and constraints on the moments themselves. This paper\ndevelops an algorithm to solve this problem with sequential convex programming,\nand it is demonstrated through a two-body and three-body example. The examples\nshow that individual moments can be directly controlled, and the moments are\naccurately approximated for non-Gaussian distributions throughout the\ncontroller's time horizon in nonlinear dynamics.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.12954", "categories": ["cs.CV", "68T07, 68U10", "I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.12954", "abs": "https://arxiv.org/abs/2510.12954", "authors": ["Denis Rychkovskiy", "GPT-5"], "title": "CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models", "comment": "8 pages, 3 figures. Endorsed by Dr. Seyedmorteza Sadat (ETH Zurich).\n  The work introduces CADE 2.5 with ZeResFDG as a practical inference-time\n  guidance stack for SD/SDXL. Code and visual examples to be released on GitHub\n  and Hugging Face", "summary": "We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level\nguidance stack for SD/SDXL latent diffusion models. The central module,\nZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and\nhigh-frequency components of the guidance signal, (ii) energy rescaling that\nmatches the per-sample magnitude of the guided prediction to the positive\nbranch, and (iii) zero-projection that removes the component parallel to the\nunconditional direction. A lightweight spectral EMA with hysteresis switches\nbetween a conservative and a detail-seeking mode as structure crystallizes\nduring sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt\nadherence, and artifact control at moderate guidance scales without any\nretraining. In addition, we employ a training-free inference-time stabilizer,\nQSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail\ninjection), which improves robustness and yields natural high-frequency\nmicro-texture at high resolutions with negligible overhead. For completeness we\nnote that the same rule is compatible with alternative parameterizations (e.g.,\nvelocity), which we briefly discuss in the Appendix; however, this paper\nfocuses on SD/SDXL latent diffusion models.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.12974", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12974", "abs": "https://arxiv.org/abs/2510.12974", "authors": ["Tianyu Zhang", "Suyuchen Wang", "Chao Wang", "Juan Rodriguez", "Ahmed Masry", "Xiangru Jian", "Yoshua Bengio", "Perouz Taslakian"], "title": "Scope: Selective Cross-modal Orchestration of Visual Perception Experts", "comment": "14 pages, 2 figures", "summary": "Vision-language models (VLMs) benefit from multiple vision encoders, but\nnaively stacking them yields diminishing returns while multiplying inference\ncosts. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that\ndynamically selects one specialized encoder per image-text pair via\ninstance-level routing, unlike token-level routing in traditional MoE. SCOPE\nmaintains a shared encoder and a pool of routed encoders. A lightweight router\nuses cross-attention between text prompts and shared visual features to select\nthe optimal encoder from the routed encoders. To train this router, we\nintroduce dual entropy regularization with auxiliary losses to balance\ndataset-level load distribution with instance-level routing confidence.\nRemarkably, SCOPE with one shared plus one routed encoder outperforms models\nusing all four extra encoders simultaneously, while reducing compute by\n24-49\\%. This demonstrates that intelligent encoder selection beats brute-force\naggregation, challenging the prevailing paradigm in multi-encoder VLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13016", "abs": "https://arxiv.org/abs/2510.13016", "authors": ["Tanveer Hannan", "Shuaicong Wu", "Mark Weber", "Suprosanna Shit", "Jindong Gu", "Rajat Koner", "Aljo\u0161a O\u0161ep", "Laura Leal-Taix\u00e9", "Thomas Seidl"], "title": "SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding", "comment": null, "summary": "Understanding fine-grained actions and accurately localizing their\ncorresponding actors in space and time are fundamental capabilities for\nadvancing next-generation AI systems, including embodied agents, autonomous\nplatforms, and human-AI interaction frameworks. Despite recent progress in\nvideo understanding, existing methods predominantly address either\ncoarse-grained action recognition or generic object tracking, thereby\noverlooking the challenge of jointly detecting and tracking multiple objects\naccording to their actions while grounding them temporally. To address this\ngap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task\nthat requires models to simultaneously detect, track, and temporally localize\nall referent objects in videos based on natural language descriptions of their\nactions. To support this task, we construct SVAG-Bench, a large-scale benchmark\ncomprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering\na diverse range of objects, actions, and real-world scenes. We further propose\nSVAGFormer, a baseline framework that adapts state of the art vision language\nmodels for joint spatial and temporal grounding, and introduce SVAGEval, a\nstandardized evaluation toolkit for fair and reproducible benchmarking.\nEmpirical results show that existing models perform poorly on SVAG,\nparticularly in dense or complex scenes, underscoring the need for more\nadvanced reasoning over fine-grained object-action interactions in long videos.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13046", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13046", "abs": "https://arxiv.org/abs/2510.13046", "authors": ["Huawei Jiang", "Husna Mutahira", "Gan Huang", "Mannan Saeed Muhammad"], "title": "One Dimensional CNN ECG Mamba for Multilabel Abnormality Classification in 12 Lead ECG", "comment": "6 Pages, 2 figures", "summary": "Accurate detection of cardiac abnormalities from electrocardiogram recordings\nis regarded as essential for clinical diagnostics and decision support.\nTraditional deep learning models such as residual networks and transformer\narchitectures have been applied successfully to this task, but their\nperformance has been limited when long sequential signals are processed.\nRecently, state space models have been introduced as an efficient alternative.\nIn this study, a hybrid framework named One Dimensional Convolutional Neural\nNetwork Electrocardiogram Mamba is introduced, in which convolutional feature\nextraction is combined with Mamba, a selective state space model designed for\neffective sequence modeling. The model is built upon Vision Mamba, a\nbidirectional variant through which the representation of temporal dependencies\nin electrocardiogram data is enhanced. Comprehensive experiments on the\nPhysioNet Computing in Cardiology Challenges of 2020 and 2021 were conducted,\nand superior performance compared with existing methods was achieved.\nSpecifically, the proposed model achieved substantially higher AUPRC and AUROC\nscores than those reported by the best previously published algorithms on\ntwelve lead electrocardiograms. These results demonstrate the potential of\nMamba-based architectures to advance reliable ECG classification. This\ncapability supports early diagnosis and personalized treatment, while enhancing\naccessibility in telemedicine and resource-constrained healthcare systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13024", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.13024", "abs": "https://arxiv.org/abs/2510.13024", "authors": ["Shahab Ataei", "Dipankar Maity", "Debdipta Goswami"], "title": "Data to Certificate: Guaranteed Cost Control with Quantization-Aware System Identification", "comment": "8 pages, 3 figures", "summary": "Cloud-assisted system identification and control have emerged as practical\nsolutions for low-power, resource-constrained control systems such as\nmicro-UAVs. In a typical cloud-assisted setting, state and input data are\ntransmitted from local agents to a central computer over low-bandwidth wireless\nlinks, leading to quantization. This paper investigates the impact of state and\ninput data quantization on a linear time invariant (LTI) system identification,\nderives a worst-case bound on the identification error, and develops a robust\ncontroller for guaranteed cost control. We establish a fundamental bound on the\nmodel error that depends only on the quantized data and quantization\nresolution, and develop a linear matrix inequality (LMI) based guaranteed cost\nrobust controller under this error bound.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13262", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13262", "abs": "https://arxiv.org/abs/2510.13262", "authors": ["Weiqi Guo", "Guanjun Liu", "Ziyuan Zhou"], "title": "SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning", "comment": null, "summary": "Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for\ncooperative and competitive tasks such as autonomous driving and strategic\ngaming. However, models trained by MADRL are vulnerable to adversarial\nperturbations on states and actions. Therefore, it is essential to investigate\nthe robustness of MADRL models from an attack perspective. Existing studies\nfocus on either state-only attacks or action-only attacks, but do not consider\nhow to effectively joint them. Simply combining state and action perturbations\nsuch as randomly perturbing states and actions does not exploit their potential\nsynergistic effects. In this paper, we propose the State-Action Joint Attack\n(SAJA) framework that has a good synergistic effects. SAJA consists of two\nimportant phases: (1) In the state attack phase, a multi-step gradient ascent\nmethod utilizes both the actor network and the critic network to compute an\nadversarial state, and (2) in the action attack phase, based on the perturbed\nstate, a second gradient ascent uses the critic network to craft the final\nadversarial action. Additionally, a heuristic regularizer measuring the\ndistance between the perturbed actions and the original clean ones is added\ninto the loss function to enhance the effectiveness of the critic's guidance.\nWe evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating\nthat (1) it outperforms and is more stealthy than state-only or action-only\nattacks, and (2) existing state or action defense methods cannot defend its\nattacks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.12997", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12997", "abs": "https://arxiv.org/abs/2510.12997", "authors": ["Binxin Gao", "Jingjun Han"], "title": "Max It or Miss It: Benchmarking LLM On Solving Extremal Problems", "comment": "Our benchmark dataset is available at\n  https://huggingface.co/datasets/binxingao/extrem-bench", "summary": "Test-time scaling has enabled Large Language Models (LLMs) with remarkable\nreasoning capabilities, particularly in mathematical domains, through\nintermediate chain-of-thought (CoT) reasoning before generating final answers.\nHowever, the specific sources and mechanisms underlying these reasoning\ncapabilities remain insufficiently understood. Optimization reasoning, i.e.\nfinding extrema under constraints, represents a fundamental abstraction that\nunderpins critical applications in planning, control, resource allocation, and\nprompt search. To systematically evaluate this capability, we introduce\nExtremBench, a benchmark dataset for solving mathematical extremal problems,\ncurated from inequality exercises used for Chinese Mathematical Olympiad and\ntransformed into $93$ standardized extrema-finding problems. We conduct\nextensive evaluations across various state-of-the-art open-source model\nfamilies, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that\nLLMs' extremal-solving reasoning capabilities do not always align with those of\ncurrent mathematical benchmarks such as AIME25 and MATH-500, with some models\nshowing strong general mathematical reasoning but poor extremal-solving skills,\nand vice versa. This discrepancy highlights a critical gap in current\nevaluation practices and suggests that existing benchmarks may not\ncomprehensively capture the full spectrum of mathematical reasoning abilities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13060", "categories": ["cs.LG", "cs.GT", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.13060", "abs": "https://arxiv.org/abs/2510.13060", "authors": ["Anupam Nayak", "Tong Yang", "Osman Yagan", "Gauri Joshi", "Yuejie Chi"], "title": "Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games", "comment": null, "summary": "Reverse Kullback-Leibler (KL) divergence-based regularization with respect to\na fixed reference policy is widely used in modern reinforcement learning to\npreserve the desired traits of the reference policy and sometimes to promote\nexploration (using uniform reference policy, known as entropy regularization).\nBeyond serving as a mere anchor, the reference policy can also be interpreted\nas encoding prior knowledge about good actions in the environment. In the\ncontext of alignment, recent game-theoretic approaches have leveraged KL\nregularization with pretrained language models as reference policies, achieving\nnotable empirical success in self-play methods. Despite these advances, the\ntheoretical benefits of KL regularization in game-theoretic settings remain\npoorly understood. In this work, we develop and analyze algorithms that\nprovably achieve improved sample efficiency under KL regularization. We study\nboth two-player zero-sum Matrix games and Markov games: for Matrix games, we\npropose OMG, an algorithm based on best response sampling with optimistic\nbonuses, and extend this idea to Markov games through the algorithm SOMG, which\nalso uses best response sampling and a novel concept of superoptimistic\nbonuses. Both algorithms achieve a logarithmic regret in $T$ that scales\ninversely with the KL regularization strength $\\beta$ in addition to the\nstandard $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret independent of $\\beta$\nwhich is attained in both regularized and unregularized settings", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13067", "abs": "https://arxiv.org/abs/2510.13067", "authors": ["Kaixuan Yang", "Wei Xiang", "Zhenshuai Chen", "Tong Jin", "Yunpeng Liu"], "title": "Direction-aware multi-scale gradient loss for infrared and visible image fusion", "comment": null, "summary": "Infrared and visible image fusion aims to integrate complementary information\nfrom co-registered source images to produce a single, informative result. Most\nlearning-based approaches train with a combination of structural similarity\nloss, intensity reconstruction loss, and a gradient-magnitude term. However,\ncollapsing gradients to their magnitude removes directional information,\nyielding ambiguous supervision and suboptimal edge fidelity. We introduce a\ndirection-aware, multi-scale gradient loss that supervises horizontal and\nvertical components separately and preserves their sign across scales. This\naxis-wise, sign-preserving objective provides clear directional guidance at\nboth fine and coarse resolutions, promoting sharper, better-aligned edges and\nricher texture preservation without changing model architectures or training\nprotocols. Experiments on open-source model and multiple public benchmarks\ndemonstrate effectiveness of our approach.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13065", "categories": ["cs.LG", "stat.ML", "62H30", "I.5.3"], "pdf": "https://arxiv.org/pdf/2510.13065", "abs": "https://arxiv.org/abs/2510.13065", "authors": ["Adil M. Bagirov", "Ramiz M. Aliguliyev", "Nargiz Sultanova", "Sona Taheri"], "title": "Absolute indices for determining compactness, separability and number of clusters", "comment": "25 pages, 11 figures, 9 tables", "summary": "Finding \"true\" clusters in a data set is a challenging problem. Clustering\nsolutions obtained using different models and algorithms do not necessarily\nprovide compact and well-separated clusters or the optimal number of clusters.\nCluster validity indices are commonly applied to identify such clusters.\nNevertheless, these indices are typically relative, and they are used to\ncompare clustering algorithms or choose the parameters of a clustering\nalgorithm. Moreover, the success of these indices depends on the underlying\ndata structure. This paper introduces novel absolute cluster indices to\ndetermine both the compactness and separability of clusters. We define a\ncompactness function for each cluster and a set of neighboring points for\ncluster pairs. This function is utilized to determine the compactness of each\ncluster and the whole cluster distribution. The set of neighboring points is\nused to define the margin between clusters and the overall distribution margin.\nThe proposed compactness and separability indices are applied to identify the\ntrue number of clusters. Using a number of synthetic and real-world data sets,\nwe demonstrate the performance of these new indices and compare them with other\nwidely-used cluster validity indices.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13075", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13075", "abs": "https://arxiv.org/abs/2510.13075", "authors": ["Hoda Kalabizadeh", "Ludovica Griffanti", "Pak-Hei Yeung", "Ana I. L. Namburete", "Nicola K. Dinsdale", "Konstantinos Kamnitsas"], "title": "Unsupervised Domain Adaptation via Content Alignment for Hippocampus Segmentation", "comment": null, "summary": "Deep learning models for medical image segmentation often struggle when\ndeployed across different datasets due to domain shifts - variations in both\nimage appearance, known as style, and population-dependent anatomical\ncharacteristics, referred to as content. This paper presents a novel\nunsupervised domain adaptation framework that directly addresses domain shifts\nencountered in cross-domain hippocampus segmentation from MRI, with specific\nemphasis on content variations. Our approach combines efficient style\nharmonisation through z-normalisation with a bidirectional deformable image\nregistration (DIR) strategy. The DIR network is jointly trained with\nsegmentation and discriminator networks to guide the registration with respect\nto a region of interest and generate anatomically plausible transformations\nthat align source images to the target domain. We validate our approach through\ncomprehensive evaluations on both a synthetic dataset using Morpho-MNIST (for\ncontrolled validation of core principles) and three MRI hippocampus datasets\nrepresenting populations with varying degrees of atrophy. Across all\nexperiments, our method outperforms existing baselines. For hippocampus\nsegmentation, when transferring from young, healthy populations to clinical\ndementia patients, our framework achieves up to 15% relative improvement in\nDice score compared to standard augmentation methods, with the largest gains\nobserved in scenarios with substantial content shift. These results highlight\nthe efficacy of our approach for accurate hippocampus segmentation across\ndiverse populations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13279", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.13279", "abs": "https://arxiv.org/abs/2510.13279", "authors": ["Fuma Omori", "Atsushi Yano", "Takuya Azumi"], "title": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution Time", "comment": null, "summary": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13459", "categories": ["cs.AI", "cs.CE", "cs.NI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.13459", "abs": "https://arxiv.org/abs/2510.13459", "authors": ["Timothy Wong", "Tom Freeman", "Joseph Feehily"], "title": "Mobile Coverage Analysis using Crowdsourced Data", "comment": "8 pages", "summary": "Effective assessment of mobile network coverage and the precise\nidentification of service weak spots are paramount for network operators\nstriving to enhance user Quality of Experience (QoE). This paper presents a\nnovel framework for mobile coverage and weak spot analysis utilising\ncrowdsourced QoE data. The core of our methodology involves coverage analysis\nat the individual cell (antenna) level, subsequently aggregated to the site\nlevel, using empirical geolocation data. A key contribution of this research is\nthe application of One-Class Support Vector Machine (OC-SVM) algorithm for\ncalculating mobile network coverage. This approach models the decision\nhyperplane as the effective coverage contour, facilitating robust calculation\nof coverage areas for individual cells and entire sites. The same methodology\nis extended to analyse crowdsourced service loss reports, thereby identifying\nand quantifying geographically localised weak spots. Our findings demonstrate\nthe efficacy of this novel framework in accurately mapping mobile coverage and,\ncrucially, in highlighting granular areas of signal deficiency, particularly\nwithin complex urban environments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13023", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.13023", "abs": "https://arxiv.org/abs/2510.13023", "authors": ["Joshua R. Tempelman", "Adam J. Wachtor", "Eric B. Flynn"], "title": "Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment", "comment": "26 pages, 6 page appendix", "summary": "Automated ultrasonic weld inspection remains a significant challenge in the\nnondestructive evaluation (NDE) community to factors such as limited training\ndata (due to the complexity of curating experimental specimens or high-fidelity\nsimulations) and environmental volatility of many industrial settings\n(resulting in the corruption of on-the-fly measurements). Thus, an end-to-end\nmachine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,\nindustrial) settings has remained an elusive goal. This work addresses the\nchallenges of data curation and signal corruption by proposing workflow\nconsisting of a reduced-order modeling scheme, diffusion based distribution\nalignment, and U-Net-based segmentation and inversion. A reduced-order\nHelmholtz model based on Lamb wave theory is used to generate a comprehensive\ndataset over varying weld heterogeneity and crack defects. The relatively\ninexpensive low-order solutions provide a robust training dateset for inversion\nmodels which are refined through a transfer learning stage using a limited set\nof full 3D elastodynamic simulations. To handle out-of-distribution (OOD)\nreal-world measurements with varying and unpredictable noise distributions,\ni.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distribution\nrepresentations of OOD experimental LDV scans which are subsequently processed\nby the inversion models. This integrated framework provides an end-to-end\nsolution for automated weld inspection on real data.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13112", "categories": ["cs.LG", "hep-lat", "physics.comp-ph", "stat.ML", "82B20, 65C05, 68T07, 60J22", "G.3; I.6.8; J.2"], "pdf": "https://arxiv.org/pdf/2510.13112", "abs": "https://arxiv.org/abs/2510.13112", "authors": ["Andrey Bryutkin", "Youssef Marzouk"], "title": "Neural Triangular Transport Maps: A New Approach Towards Sampling in Lattice QCD", "comment": null, "summary": "Lattice field theories are fundamental testbeds for computational physics;\nyet, sampling their Boltzmann distributions remains challenging due to\nmultimodality and long-range correlations. While normalizing flows offer a\npromising alternative, their application to large lattices is often constrained\nby prohibitive memory requirements and the challenge of maintaining sufficient\nmodel expressivity. We propose sparse triangular transport maps that explicitly\nexploit the conditional independence structure of the lattice graph under\nperiodic boundary conditions using monotone rectified neural networks (MRNN).\nWe introduce a comprehensive framework for triangular transport maps that\nnavigates the fundamental trade-off between \\emph{exact sparsity} (respecting\nmarginal conditional independence in the target distribution) and\n\\emph{approximate sparsity} (computational tractability without fill-ins).\nRestricting each triangular map component to a local past enables site-wise\nparallel evaluation and linear time complexity in lattice size $N$, while\npreserving the expressive, invertible structure. Using $\\phi^4$ in two\ndimensions as a controlled setting, we analyze how node labelings (orderings)\naffect the sparsity and performance of triangular maps. We compare against\nHybrid Monte Carlo (HMC) and established flow approaches (RealNVP).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13084", "abs": "https://arxiv.org/abs/2510.13084", "authors": ["Yi Zuo", "Zitao Wang", "Lingling Li", "Xu Liu", "Fang Liu", "Licheng Jiao"], "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation", "comment": "32 pages, 11 figures", "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13449", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.13449", "abs": "https://arxiv.org/abs/2510.13449", "authors": ["Jan Br\u00e4ndle", "Julie Rousseau", "Pulkit Nahata", "Gabriela Hug"], "title": "On the Flexibility Potential of a Swiss Distribution Grid: Opportunities and Limitations", "comment": null, "summary": "The growing integration of distributed renewable generation and the\nelectrification of heating and transportation are rapidly increasing the number\nof flexible devices within modern distribution grids. Leveraging the aggregated\nflexibility of these small-scale distributed resources is essential to\nmaintaining future grid-wide stability. This work uses the Swiss distribution\ngrid of Walenstadt as a case study to provide insights into the aggregated\nflexibility potential of distribution grids. It demonstrates that incorporating\ndevices such as heat pumps and photovoltaic systems significantly enhances\ndistribution grid flexibility. It investigates the time-varying nature of\naggregated flexibility and highlights how it can vary seasonally. Furthermore,\nsimulations of future scenarios reveal that aggregated flexibility does not\nincrease linearly or monotonically with higher levels of flexible device\npenetration. This is primarily due to the overloading of individual feeders,\nwhich underscores the impact of grid topology and network constraints on the\naggregated flexibility potential.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13105", "abs": "https://arxiv.org/abs/2510.13105", "authors": ["Xijun Wang", "Tanay Sharma", "Achin Kulshrestha", "Abhimitra Meka", "Aveek Purohit", "Dinesh Manocha"], "title": "EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception", "comment": null, "summary": "As AR/VR technologies become integral to daily life, there's a growing need\nfor AI that understands human social dynamics from an egocentric perspective.\nHowever, current LLMs often lack the social awareness to discern when to\nintervene as AI assistant. This leads to constant, socially unaware responses\nthat may disrupt natural conversation and negatively impact user focus. To\naddress these limitations, we introduce EgoSocial, a large-scale egocentric\ndataset with 13,500 social video-question pairs, specifically designed to\nbenchmark intervention in social interaction perception. We also present an\nin-depth analysis of current omnimodal LLMs (OLLMs) to assess their\neffectiveness in detecting diverse social contextual cues. Experiments show\nthat OLLMs still struggle to detect the intervention timing (14.4% for Gemini\n2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method\nfor robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD\nintegrates multimodal contextual cues (e.g., audio and visual cues) into a\nsocial thinking graph, dynamically modeling participants and interactions. Our\nmethod proactively detects intervention timing and social interactions,\nprecisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and\nGemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4\nby 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.\nWe will release the dataset and code soon.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13461", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.13461", "abs": "https://arxiv.org/abs/2510.13461", "authors": ["Yangye Jiang", "Jiachen Wang", "Daofei Li"], "title": "Physics-Informed Neural Network Modeling of Vehicle Collision Dynamics in Precision Immobilization Technique Maneuvers", "comment": null, "summary": "Accurate prediction of vehicle collision dynamics is crucial for advanced\nsafety systems and post-impact control applications, yet existing methods face\ninherent trade-offs among computational efficiency, prediction accuracy, and\ndata requirements. This paper proposes a dual Physics-Informed Neural Network\nframework addressing these challenges through two complementary networks. The\nfirst network integrates Gaussian Mixture Models with PINN architecture to\nlearn impact force distributions from finite element analysis data while\nenforcing momentum conservation and energy consistency constraints. The second\nnetwork employs an adaptive PINN with dynamic constraint weighting to predict\npost-collision vehicle dynamics, featuring an adaptive physics guard layer that\nprevents unrealistic predictions whil e preserving data-driven learning\ncapabilities. The framework incorporates uncertainty quantification through\ntime-varying parameters and enables rapid adaptation via fine-tuning\nstrategies. Validation demonstrates significant improvements: the impact force\nmodel achieves relative errors below 15.0% for force prediction on finite\nelement analysis (FEA) datasets, while the vehicle dynamics model reduces\naverage trajectory prediction error by 63.6% compared to traditional\nfour-degree-of-freedom models in scaled vehicle experiments. The integrated\nsystem maintains millisecond-level computational efficiency suitable for\nreal-time applications while providing probabilistic confidence bounds\nessential for safety-critical control. Comprehensive validation through FEA\nsimulation, dynamic modeling, and scaled vehicle experiments confirms the\nframework's effectiveness for Precision Immobilization Technique scenarios and\ngeneral collision dynamics prediction.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13108", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13108", "abs": "https://arxiv.org/abs/2510.13108", "authors": ["Jingyu Song", "Zhenxin Li", "Shiyi Lan", "Xinglong Sun", "Nadine Chang", "Maying Shen", "Joshua Chen", "Katherine A. Skinner", "Jose M. Alvarez"], "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models", "comment": "9 pages, 3 figures", "summary": "Benchmarking autonomous driving planners to align with human judgment remains\na critical challenge, as state-of-the-art metrics like the Extended Predictive\nDriver Model Score (EPDMS) lack context awareness in nuanced scenarios. To\naddress this, we introduce DriveCritic, a novel framework featuring two key\ncontributions: the DriveCritic dataset, a curated collection of challenging\nscenarios where context is critical for correct judgment and annotated with\npairwise human preferences, and the DriveCritic model, a Vision-Language Model\n(VLM) based evaluator. Fine-tuned using a two-stage supervised and\nreinforcement learning pipeline, the DriveCritic model learns to adjudicate\nbetween trajectory pairs by integrating visual and symbolic context.\nExperiments show DriveCritic significantly outperforms existing metrics and\nbaselines in matching human preferences and demonstrates strong context\nawareness. Overall, our work provides a more reliable, human-aligned foundation\nto evaluating autonomous driving systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13514", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.13514", "abs": "https://arxiv.org/abs/2510.13514", "authors": ["Andreas C. Makrides", "Adam Suski", "Elina Spyrou"], "title": "Quantifying the Impact of Missing Risk Markets for Decarbonized Power Systems with Long Duration Energy Storage", "comment": null, "summary": "The transition to a fully decarbonised electricity system depends on\nintegrating new technologies that ensure reliability alongside sustainability.\nHowever, missing risk markets hinder investment in reliability-enhancing\ntechnologies by exposing investors to revenue uncertainty. This study provides\nthe first quantitative assessment of how missing risk markets affect investment\ndecisions in power systems that depend on long-duration energy storage (LDES)\nfor reliability. We develop a two-stage stochastic equilibrium model with\nrisk-averse market participants, which independently sizes power and energy\ncapacity. We apply the method to a case study of a deeply decarbonised power\nsystem in Great Britain. The results show that incomplete risk markets reduce\nsocial welfare, harm reliability, and discourage investment in LDES and other\ntechnologies with volatile revenue streams. Revenue volatility leads to\nsubstantial risk premiums and higher financing costs for LDES, creating a\nbarrier to its large-scale deployment. These findings demonstrate the\nimportance of policy mechanisms that hedge revenue risk to lower the cost of\ncapital and accelerate investment in reliability-enhancing, zero-carbon\ntechnologies", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13691", "abs": "https://arxiv.org/abs/2510.13691", "authors": ["Cecilia Di Florio", "Huimin Dong", "Antonino Rotolo"], "title": "A Modal Logic for Temporal and Jurisdictional Classifier Models", "comment": "18 pages, 2 figures. Extended version of a short paper accepted at\n  PRIMA 2025. This is the authors' version of the work. It is posted here for\n  your personal use", "summary": "Logic-based models can be used to build verification tools for machine\nlearning classifiers employed in the legal field. ML classifiers predict the\noutcomes of new cases based on previous ones, thereby performing a form of\ncase-based reasoning (CBR). In this paper, we introduce a modal logic of\nclassifiers designed to formally capture legal CBR. We incorporate principles\nfor resolving conflicts between precedents, by introducing into the logic the\ntemporal dimension of cases and the hierarchy of courts within the legal\nsystem.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13050", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.13050", "abs": "https://arxiv.org/abs/2510.13050", "authors": ["Shreya Agrawal", "Mohammed Alewi Hassen", "Emmanuel Asiedu Brempong", "Boris Babenko", "Fred Zyda", "Olivia Graham", "Di Li", "Samier Merchant", "Santiago Hincapie Potes", "Tyler Russell", "Danny Cheresnick", "Aditya Prakash Kakkirala", "Stephan Rasp", "Avinatan Hassidim", "Yossi Matias", "Nal Kalchbrenner", "Pramod Gupta", "Jason Hickey", "Aaron Bell"], "title": "An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting", "comment": null, "summary": "Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is\na critical tool for vulnerable communities in the Global South frequently\nexposed to intense, rapidly developing storms. Timely forecasts provide a\ncrucial window to protect lives and livelihoods. Traditional numerical weather\nprediction (NWP) methods suffer from high latency, low spatial and temporal\nresolution, and significant gaps in accuracy across the world. Recent machine\nlearning-based nowcasting methods, common in the Global North, cannot be\nextended to the Global South due to extremely sparse radar coverage. We present\nGlobal MetNet, an operational global machine learning nowcasting model. It\nleverages the Global Precipitation Mission's CORRA dataset, geostationary\nsatellite data, and global NWP data to predict precipitation for the next 12\nhours. The model operates at a high resolution of approximately 0.05{\\deg}\n(~5km) spatially and 15 minutes temporally. Global MetNet significantly\noutperforms industry-standard hourly forecasts and achieves significantly\nhigher skill, making forecasts useful over a much larger area of the world than\npreviously available. Our model demonstrates better skill in data-sparse\nregions than even the best high-resolution NWP models achieve in the US.\nValidated using ground radar and satellite data, it shows significant\nimprovements across key metrics like the critical success index and fractions\nskill score for all precipitation rates and lead times. Crucially, our model\ngenerates forecasts in under a minute, making it readily deployable for\nreal-time applications. It is already deployed for millions of users on Google\nSearch. This work represents a key step in reducing global disparities in\nforecast quality and integrating sparse, high-resolution satellite observations\ninto weather forecasting.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13709", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13709", "abs": "https://arxiv.org/abs/2510.13709", "authors": ["Evan Ellis", "Vivek Myers", "Jens Tuyls", "Sergey Levine", "Anca Dragan", "Benjamin Eysenbach"], "title": "Training LLM Agents to Empower Humans", "comment": null, "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13052", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SP", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.13052", "abs": "https://arxiv.org/abs/2510.13052", "authors": ["Muhammad Faraz Ul Abrar", "Nicol\u00f2 Michelusi", "Erik G. Larsson"], "title": "Time-Varying Optimization for Streaming Data Via Temporal Weighting", "comment": "Accepted at IEEE Asilomar, 2025", "summary": "Classical optimization theory deals with fixed, time-invariant objective\nfunctions. However, time-varying optimization has emerged as an important\nsubject for decision-making in dynamic environments. In this work, we study the\nproblem of learning from streaming data through a time-varying optimization\nlens. Unlike prior works that focus on generic formulations, we introduce a\nstructured, \\emph{weight-based} formulation that explicitly captures the\nstreaming-data origin of the time-varying objective, where at each time step,\nan agent aims to minimize a weighted average loss over all the past data\nsamples. We focus on two specific weighting strategies: (1) uniform weights,\nwhich treat all samples equally, and (2) discounted weights, which\ngeometrically decay the influence of older data. For both schemes, we derive\ntight bounds on the ``tracking error'' (TE), defined as the deviation between\nthe model parameter and the time-varying optimum at a given time step, under\ngradient descent (GD) updates. We show that under uniform weighting, the TE\nvanishes asymptotically with a $\\mathcal{O}(1/t)$ decay rate, whereas\ndiscounted weighting incurs a nonzero error floor controlled by the discount\nfactor and the number of gradient updates performed at each time step. Our\ntheoretical findings are validated through numerical simulations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13727", "abs": "https://arxiv.org/abs/2510.13727", "authors": ["Ravi Pandya", "Madison Bland", "Duy P. Nguyen", "Changliu Liu", "Jaime Fern\u00e1ndez Fisac", "Andrea Bajcsy"], "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails", "comment": null, "summary": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13744", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13744", "abs": "https://arxiv.org/abs/2510.13744", "authors": ["Shrey Pandit", "Austin Xu", "Xuan-Phi Nguyen", "Yifei Ming", "Caiming Xiong", "Shafiq Joty"], "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math", "comment": "21 pages, 8 figures, 5 tables", "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13160", "abs": "https://arxiv.org/abs/2510.13160", "authors": ["Meng Yang", "Kecheng Chen", "Wei Luo", "Xianjie Chen", "Yong Jia", "Mingyue Wang", "Fanqiang Lin"], "title": "DP-TTA: Test-time Adaptation for Transient Electromagnetic Signal Denoising via Dictionary-driven Prior Regularization", "comment": null, "summary": "Transient Electromagnetic (TEM) method is widely used in various geophysical\napplications, providing valuable insights into subsurface properties. However,\ntime-domain TEM signals are often submerged in various types of noise. While\nrecent deep learning-based denoising models have shown strong performance,\nthese models are mostly trained on simulated or single real-world scenario\ndata, overlooking the significant differences in noise characteristics from\ndifferent geographical regions. Intuitively, models trained in one environment\noften struggle to perform well in new settings due to differences in geological\nconditions, equipment, and external interference, leading to reduced denoising\nperformance. To this end, we propose the Dictionary-driven Prior Regularization\nTest-time Adaptation (DP-TTA). Our key insight is that TEM signals possess\nintrinsic physical characteristics, such as exponential decay and smoothness,\nwhich remain consistent across different regions regardless of external\nconditions. These intrinsic characteristics serve as ideal prior knowledge for\nguiding the TTA strategy, which helps the pre-trained model dynamically adjust\nparameters by utilizing self-supervised losses, improving denoising performance\nin new scenarios. To implement this, we customized a network, named DTEMDNet.\nSpecifically, we first use dictionary learning to encode these intrinsic\ncharacteristics as a dictionary-driven prior, which is integrated into the\nmodel during training. At the testing stage, this prior guides the model to\nadapt dynamically to new environments by minimizing self-supervised losses\nderived from the dictionary-driven consistency and the signal one-order\nvariation. Extensive experimental results demonstrate that the proposed method\nachieves much better performance than existing TEM denoising methods and TTA\nmethods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13186", "abs": "https://arxiv.org/abs/2510.13186", "authors": ["Zhen Li", "Xibin Jin", "Guoliang Li", "Shuai Wang", "Miaowen Wen", "Huseyin Arslan", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control", "comment": null, "summary": "Edge Gaussian splatting (EGS), which aggregates data from distributed clients\nand trains a global GS model at the edge server, is an emerging paradigm for\nscene reconstruction. Unlike traditional edge resource management methods that\nemphasize communication throughput or general-purpose learning performance, EGS\nexplicitly aims to maximize the GS qualities, rendering existing approaches\ninapplicable. To address this problem, this paper formulates a novel\nGS-oriented objective function that distinguishes the heterogeneous view\ncontributions of different clients. However, evaluating this function in turn\nrequires clients' images, leading to a causality dilemma. To this end, this\npaper further proposes a sample-then-transmit EGS (or STT-GS for short)\nstrategy, which first samples a subset of images as pilot data from each client\nfor loss prediction. Based on the first-stage evaluation, communication\nresources are then prioritized towards more valuable clients. To achieve\nefficient sampling, a feature-domain clustering (FDC) scheme is proposed to\nselect the most representative data and pilot transmission time minimization\n(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint\nclient selection and power control (JCSPC) framework to maximize the\nGS-oriented function under communication resource constraints. Despite the\nnonconvexity of the problem, we propose a low-complexity efficient solution\nbased on the penalty alternating majorization minimization (PAMM) algorithm.\nExperiments unveil that the proposed scheme significantly outperforms existing\nbenchmarks on real-world datasets. It is found that the GS-oriented objective\ncan be accurately predicted with low sampling ratios (e.g.,10%), and our method\nachieves an excellent tradeoff between view contributions and communication\ncosts.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13198", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13198", "abs": "https://arxiv.org/abs/2510.13198", "authors": ["Rongtao Xu", "Jinzhou Lin", "Jialei Zhou", "Jiahua Dong", "Changwei Wang", "Ruisheng Wang", "Li Guo", "Shibiao Xu", "Xiaodan Liang"], "title": "Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion", "comment": null, "summary": "Camera-based occupancy prediction is a mainstream approach for 3D perception\nin autonomous driving, aiming to infer complete 3D scene geometry and semantics\nfrom 2D images. Almost existing methods focus on improving performance through\nstructural modifications, such as lightweight backbones and complex cascaded\nframeworks, with good yet limited performance. Few studies explore from the\nperspective of representation fusion, leaving the rich diversity of features in\n2D images underutilized. Motivated by this, we propose \\textbf{CIGOcc, a\ntwo-stage occupancy prediction framework based on multi-level representation\nfusion. \\textbf{CIGOcc extracts segmentation, graphics, and depth features from\nan input image and introduces a deformable multi-level fusion mechanism to fuse\nthese three multi-level features. Additionally, CIGOcc incorporates knowledge\ndistilled from SAM to further enhance prediction accuracy. Without increasing\ntraining costs, CIGOcc achieves state-of-the-art performance on the\nSemanticKITTI benchmark. The code is provided in the supplementary material and\nwill be released https://github.com/VitaLemonTea1/CIGOcc", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13208", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13208", "abs": "https://arxiv.org/abs/2510.13208", "authors": ["Lianlian Liu", "YongKang He", "Zhaojie Chu", "Xiaofen Xing", "Xiangmin Xu"], "title": "MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation", "comment": null, "summary": "Generating stylized 3D human motion from speech signals presents substantial\nchallenges, primarily due to the intricate and fine-grained relationships among\nspeech signals, individual styles, and the corresponding body movements.\nCurrent style encoding approaches either oversimplify stylistic diversity or\nignore regional motion style differences (e.g., upper vs. lower body), limiting\nmotion realism. Additionally, motion style should dynamically adapt to changes\nin speech rhythm and emotion, but existing methods often overlook this. To\naddress these issues, we propose MimicParts, a novel framework designed to\nenhance stylized motion generation based on part-aware style injection and\npart-aware denoising network. It divides the body into different regions to\nencode localized motion styles, enabling the model to capture fine-grained\nregional differences. Furthermore, our part-aware attention block allows rhythm\nand emotion cues to guide each body region precisely, ensuring that the\ngenerated motion aligns with variations in speech rhythm and emotional state.\nExperimental results show that our method outperforming existing methods\nshowcasing naturalness and expressive 3D human motion sequences.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13219", "abs": "https://arxiv.org/abs/2510.13219", "authors": ["Xi Xiao", "Yunbei Zhang", "Lin Zhao", "Yiyang Liu", "Xiaoying Liao", "Zheda Mai", "Xingjian Li", "Xiao Wang", "Hao Xu", "Jihun Hamm", "Xue Lin", "Min Xu", "Qifan Wang", "Tianyang Wang", "Cheng Han"], "title": "Prompt-based Adaptation in Large-scale Vision Models: A Survey", "comment": null, "summary": "In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have\nrecently emerged as lightweight and effective alternatives to full fine-tuning\nfor adapting large-scale vision models within the ``pretrain-then-finetune''\nparadigm. However, despite rapid progress, their conceptual boundaries remain\nblurred, as VP and VPT are frequently used interchangeably in current research,\nreflecting a lack of systematic distinction between these techniques and their\nrespective applications. In this survey, we revisit the designs of VP and VPT\nfrom first principles, and conceptualize them within a unified framework termed\nPrompt-based Adaptation (PA). We provide a taxonomy that categorizes existing\nmethods into learnable, generative, and non-learnable prompts, and further\norganizes them by injection granularity -- pixel-level and token-level. Beyond\nthe core methodologies, we examine PA's integrations across diverse domains,\nincluding medical imaging, 3D point clouds, and vision-language tasks, as well\nas its role in test-time adaptation and trustworthy AI. We also summarize\ncurrent benchmarks and identify key challenges and future directions. To the\nbest of our knowledge, we are the first comprehensive survey dedicated to PA's\nmethodologies and applications in light of their distinct characteristics. Our\nsurvey aims to provide a clear roadmap for researchers and practitioners in all\narea to understand and explore the evolving landscape of PA-related research.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13226", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13226", "abs": "https://arxiv.org/abs/2510.13226", "authors": ["Hang-Cheng Dong", "Yibo Jiao", "Fupeng Wei", "Guodong Liu", "Dong Ye", "Bingguo Liu"], "title": "Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects", "comment": null, "summary": "Industrial surface defect inspection for sample-wise quality control (QC)\nmust simultaneously decide whether a given sample contains defects and localize\nthose defects spatially. In real production lines, extreme\nforeground-background imbalance, defect sparsity with a long-tailed scale\ndistribution, and low contrast are common. As a result, pixel-centric training\nand evaluation are easily dominated by large homogeneous regions, making it\ndifficult to drive models to attend to small or low-contrast defects-one of the\nmain bottlenecks for deployment. Empirically, existing models achieve strong\npixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the\nsample level, especially for sparse or slender defects. The root cause is a\nmismatch between the optimization objective and the granularity of QC\ndecisions. To address this, we propose a sample-centric multi-task learning\nframework and evaluation suite. Built on a shared-encoder architecture, the\nmethod jointly learns sample-level defect classification and pixel-level mask\nlocalization. Sample-level supervision modulates the feature distribution and,\nat the gradient level, continually boosts recall for small and low-contrast\ndefects, while the segmentation branch preserves boundary and shape details to\nenhance per-sample decision stability and reduce misses. For evaluation, we\npropose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias\nof classical mIoU caused by empty or true-negative samples and tightly couple\nlocalization quality with sample-level decisions. Experiments on two benchmark\ndatasets demonstrate that our approach substantially improves the reliability\nof sample-level decisions and the completeness of defect localization.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13232", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13232", "abs": "https://arxiv.org/abs/2510.13232", "authors": ["Inha Kang", "Youngsun Lim", "Seonho Lee", "Jiho Choi", "Junsuk Choe", "Hyunjung Shim"], "title": "What \"Not\" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging", "comment": "38 pages", "summary": "State-of-the-art vision-language models (VLMs) suffer from a critical failure\nin understanding negation, often referred to as affirmative bias. This\nlimitation is particularly severe in described object detection (DOD) tasks. To\naddress this, we propose two primary contributions: (1) a new dataset pipeline\nand (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a\ndataset constructed with a systematic chain-of-thought (CoT) and VQA-based\npipeline to generate high-quality, instance-grounded negation data. Second, we\npropose NegToMe, a novel text token merging module that directly tackles the\narchitectural cause of affirmative bias. NegToMe fundamentally addresses the\nstructural loss of negation cues in tokenization, grouping them with attributes\ninto coherent semantic phrases. It maintains correct polarity at the input\nlevel, enabling robust negation understanding even with limited data. For\ninstance, to prevent a model from treating the fragmented tokens \"not\" and\n\"girl\" as simply \"girl\", NegToMe binds them into a single token whose meaning\nis correctly distinguished from that of \"girl\" alone. This module is integrated\nwith a parameter-efficient and strategic LoRA fine-tuning approach. Our method\nsignificantly improves performance on challenging negation benchmarks with a\nlowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval\nand demonstrating generalization to SoTA VLMs. This work marks a crucial step\nforward in addressing negation understanding for real-world detection\napplications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13234", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13234", "abs": "https://arxiv.org/abs/2510.13234", "authors": ["Yinglong Yan", "Jun Yue", "Shaobo Xia", "Hanmeng Sun", "Tianxu Ying", "Chengcheng Wu", "Sifan Lan", "Min He", "Pedram Ghamisi", "Leyuan Fang"], "title": "UniVector: Unified Vector Extraction via Instance-Geometry Interaction", "comment": null, "summary": "Vector extraction retrieves structured vector geometry from raster images,\noffering high-fidelity representation and broad applicability. Existing\nmethods, however, are usually tailored to a single vector type (e.g., polygons,\npolylines, line segments), requiring separate models for different structures.\nThis stems from treating instance attributes (category, structure) and\ngeometric attributes (point coordinates, connections) independently, limiting\nthe ability to capture complex structures. Inspired by the human brain's\nsimultaneous use of semantic and spatial interactions in visual perception, we\npropose UniVector, a unified VE framework that leverages instance-geometry\ninteraction to extract multiple vector types within a single model. UniVector\nencodes vectors as structured queries containing both instance- and\ngeometry-level information, and iteratively updates them through an interaction\nmodule for cross-level context exchange. A dynamic shape constraint further\nrefines global structures and key points. To benchmark multi-structure\nscenarios, we introduce the Multi-Vector dataset with diverse polygons,\npolylines, and line segments. Experiments show UniVector sets a new state of\nthe art on both single- and multi-structure VE tasks. Code and dataset will be\nreleased at https://github.com/yyyyll0ss/UniVector.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13235", "abs": "https://arxiv.org/abs/2510.13235", "authors": ["Yukuan Zhang", "Jiarui Zhao", "Shangqing Nie", "Jin Kuang", "Shengsheng Wang"], "title": "EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking", "comment": null, "summary": "Multimodal semantic cues, such as textual descriptions, have shown strong\npotential in enhancing target perception for tracking. However, existing\nmethods rely on static textual descriptions from large language models, which\nlack adaptability to real-time target state changes and prone to\nhallucinations. To address these challenges, we propose a unified multimodal\nvision-language tracking framework, named EPIPTrack, which leverages explicit\nand implicit prompts for dynamic target modeling and semantic alignment.\nSpecifically, explicit prompts transform spatial motion information into\nnatural language descriptions to provide spatiotemporal guidance. Implicit\nprompts combine pseudo-words with learnable descriptors to construct\nindividualized knowledge representations capturing appearance attributes. Both\nprompts undergo dynamic adjustment via the CLIP text encoder to respond to\nchanges in target state. Furthermore, we design a Discriminative Feature\nAugmentor to enhance visual and cross-modal representations. Extensive\nexperiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack\noutperforms existing trackers in diverse scenarios, exhibiting robust\nadaptability and superior performance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13237", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13237", "abs": "https://arxiv.org/abs/2510.13237", "authors": ["Haochuan Xu", "Yun Sing Koh", "Shuhuai Huang", "Zirun Zhou", "Di Wang", "Jun Sakuma", "Jingfeng Zhang"], "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models have achieved revolutionary progress in\nrobot learning, enabling robots to execute complex physical robot tasks from\nnatural language instructions. Despite this progress, their adversarial\nrobustness remains underexplored. In this work, we propose both adversarial\npatch attack and corresponding defense strategies for VLA models. We first\nintroduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic\nadversarial attack that generates patches directly placeable within the\ncamera's view. In comparison to prior methods, EDPA can be readily applied to\ndifferent VLA models without requiring prior knowledge of the model\narchitecture, or the controlled robotic manipulator. EDPA constructs these\npatches by (i) disrupting the semantic alignment between visual and textual\nlatent representations, and (ii) maximizing the discrepancy of latent\nrepresentations between adversarial and corresponding clean visual inputs.\nThrough the optimization of these objectives, EDPA distorts the VLA's\ninterpretation of visual information, causing the model to repeatedly generate\nincorrect actions and ultimately result in failure to complete the given\nrobotic task. To counter this, we propose an adversarial fine-tuning scheme for\nthe visual encoder, in which the encoder is optimized to produce similar latent\nrepresentations for both clean and adversarially perturbed visual inputs.\nExtensive evaluations on the widely recognized LIBERO robotic simulation\nbenchmark demonstrate that EDPA substantially increases the task failure rate\nof cutting-edge VLA models, while our proposed defense effectively mitigates\nthis degradation. The codebase is accessible via the homepage at\nhttps://edpa-attack.github.io/.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13212", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13212", "abs": "https://arxiv.org/abs/2510.13212", "authors": ["Zizhuo Zhang", "Qizhou Wang", "Shanshan Ye", "Jianing Zhu", "Jiangchao Yao", "Bo Han", "Masashi Sugiyama"], "title": "Towards Understanding Valuable Preference Data for Large Language Model Alignment", "comment": null, "summary": "Large language model (LLM) alignment is typically achieved through learning\nfrom human preference comparisons, making the quality of preference data\ncritical to its success. Existing studies often pre-process raw training\ndatasets to identify valuable preference pairs using external reward models or\noff-the-shelf LLMs, achieving improved overall performance but rarely examining\nwhether individual, selected data point is genuinely beneficial. We assess data\nquality through individual influence on validation data using our newly\nproposed truncated influence function (TIF), which mitigates the over-scoring\npresent in traditional measures and reveals that preference data quality is\ninherently a property of the model. In other words, a data pair that benefits\none model may harm another. This leaves the need to improve the preference data\nselection approaches to be adapting to specific models. To this end, we\nintroduce two candidate scoring functions (SFs) that are computationally\nsimpler than TIF and positively correlated with it. They are also model\ndependent and can serve as potential indicators of individual data quality for\npreference data selection. Furthermore, we observe that these SFs inherently\nexhibit errors when compared to TIF. To this end, we combine them to offset\ntheir diverse error sources, resulting in a simple yet effective data selection\nrule that enables the models to achieve a more precise selection of valuable\npreference data. We conduct experiments across diverse alignment benchmarks and\nvarious LLM families, with results demonstrating that better alignment\nperformance can be achieved using less data, showing the generality of our\nfindings and new methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13253", "abs": "https://arxiv.org/abs/2510.13253", "authors": ["Chunhao Lu", "Qiang Lu", "Meichen Dong", "Jake Luo"], "title": "End-to-End Multi-Modal Diffusion Mamba", "comment": "Accepted by ICCV 2025", "summary": "Current end-to-end multi-modal models utilize different encoders and decoders\nto process input and output information. This separation hinders the joint\nrepresentation learning of various modalities. To unify multi-modal processing,\nwe propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM\nutilizes a Mamba-based multi-step selection diffusion model to progressively\ngenerate and refine modality-specific information through a unified variational\nautoencoder for both encoding and decoding. This innovative approach allows MDM\nto achieve superior performance when processing high-dimensional data,\nparticularly in generating high-resolution images and extended text sequences\nsimultaneously. Our evaluations in areas such as image generation, image\ncaptioning, visual question answering, text comprehension, and reasoning tasks\ndemonstrate that MDM significantly outperforms existing end-to-end models\n(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA\nmodels like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's\neffectiveness in unifying multi-modal processes while maintaining computational\nefficiency, establishing a new direction for end-to-end multi-modal\narchitectures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13290", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13290", "abs": "https://arxiv.org/abs/2510.13290", "authors": ["Anna Hedstr\u00f6m", "Salim I. Amoukou", "Tom Bewley", "Saumitra Mishra", "Manuela Veloso"], "title": "To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models", "comment": "ICML 2025, 22 pages, 16 figures, 5 tables", "summary": "We introduce Mechanistic Error Reduction with Abstention (MERA), a principled\nframework for steering language models (LMs) to mitigate errors through\nselective, adaptive interventions. Unlike existing methods that rely on fixed,\nmanually tuned steering strengths, often resulting in under or oversteering,\nMERA addresses these limitations by (i) optimising the intervention direction,\nand (ii) calibrating when, and how much to steer, thereby provably improving\nperformance or abstaining when no confident correction is possible. Experiments\nacross diverse datasets, and LM families demonstrate safe, effective,\nnon-degrading error correction, and that MERA outperforms existing baselines.\nMoreover, MERA can be applied on top of existing steering techniques to further\nenhance their performance, establishing it as a general-purpose, and efficient\napproach to mechanistic activation steering.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13303", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.13303", "abs": "https://arxiv.org/abs/2510.13303", "authors": ["Aya Kaysan Bahjat"], "title": "Automated document processing system for government agencies using DBNET++ and BART models", "comment": "8 pages, 12 figures, article", "summary": "An automatic document classification system is presented that detects textual\ncontent in images and classifies documents into four predefined categories\n(Invoice, Report, Letter, and Form). The system supports both offline images\n(e.g., files on flash drives, HDDs, microSD) and real-time capture via\nconnected cameras, and is designed to mitigate practical challenges such as\nvariable illumination, arbitrary orientation, curved or partially occluded\ntext, low resolution, and distant text. The pipeline comprises four stages:\nimage capture and preprocessing, text detection [1] using a DBNet++\n(Differentiable Binarization Network Plus) detector, and text classification\n[2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier,\nall integrated within a user interface implemented in Python with PyQt5. The\nachieved results by the system for text detection in images were good at about\n92.88% through 10 hours on Total-Text dataset that involve high resolution\nimages simulate a various and very difficult challenges. The results indicate\nthe proposed approach is effective for practical, mixed-source document\ncategorization in unconstrained imaging scenarios.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13327", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13327", "abs": "https://arxiv.org/abs/2510.13327", "authors": ["Lina Alkarmi", "Ziyuan Huang", "Mingyan Liu"], "title": "When In Doubt, Abstain: The Impact of Abstention on Strategic Classification", "comment": null, "summary": "Algorithmic decision making is increasingly prevalent, but often vulnerable\nto strategic manipulation by agents seeking a favorable outcome. Prior research\nhas shown that classifier abstention (allowing a classifier to decline making a\ndecision due to insufficient confidence) can significantly increase classifier\naccuracy. This paper studies abstention within a strategic classification\ncontext, exploring how its introduction impacts strategic agents' responses and\nhow principals should optimally leverage it. We model this interaction as a\nStackelberg game where a principal, acting as the classifier, first announces\nits decision policy, and then strategic agents, acting as followers, manipulate\ntheir features to receive a desired outcome. Here, we focus on binary\nclassifiers where agents manipulate observable features rather than their true\nfeatures, and show that optimal abstention ensures that the principal's utility\n(or loss) is no worse than in a non-abstention setting, even in the presence of\nstrategic agents. We also show that beyond improving accuracy, abstention can\nalso serve as a deterrent to manipulation, making it costlier for agents,\nespecially those less qualified, to manipulate to achieve a positive outcome\nwhen manipulation costs are significant enough to affect agent behavior. These\nresults highlight abstention as a valuable tool for reducing the negative\neffects of strategic behavior in algorithmic decision making systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13361", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.13361", "abs": "https://arxiv.org/abs/2510.13361", "authors": ["Yisen Wang", "Yichuan Mo", "Hongjun Wang", "Junyi Li", "Zhouchen Lin"], "title": "Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training", "comment": null, "summary": "Despite the rapid progress of neural networks, they remain highly vulnerable\nto adversarial examples, for which adversarial training (AT) is currently the\nmost effective defense. While AT has been extensively studied, its practical\napplications expose two major limitations: natural accuracy tends to degrade\nsignificantly compared with standard training, and robustness does not transfer\nwell across attacks crafted under different norm constraints. Unlike prior\nworks that attempt to address only one issue within a single network, we\npropose to partition the overall generalization goal into multiple sub-tasks,\neach assigned to a dedicated base learner. By specializing in its designated\nobjective, each base learner quickly becomes an expert in its field. In the\nlater stages of training, we interpolate their parameters to form a\nknowledgeable global learner, while periodically redistributing the global\nparameters back to the base learners to prevent their optimization trajectories\nfrom drifting too far from the shared target. We term this framework Generalist\nand introduce three variants tailored to different application scenarios. Both\ntheoretical analysis and extensive experiments demonstrate that Generalist\nachieves lower generalization error and significantly alleviates the trade-off\nproblems compared with baseline methods. Our results suggest that Generalist\nprovides a promising step toward developing fully robust classifiers in the\nfuture.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13364", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13364", "abs": "https://arxiv.org/abs/2510.13364", "authors": ["MingZe Tang", "Jubal Chandy Jacob"], "title": "Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity", "comment": null, "summary": "Recent Vision-Language Models (VLMs) enable zero-shot classification by\naligning images and text in a shared space, a promising approach for\ndata-scarce conditions. However, the influence of prompt design on recognizing\nvisually similar categories, such as human postures, is not well understood.\nThis study investigates how prompt specificity affects the zero-shot\nclassification of sitting, standing, and walking/running on a small, 285-image\nCOCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,\nand SigLip, were evaluated using a three-tiered prompt design that\nsystematically increases linguistic detail. Our findings reveal a compelling,\ncounter-intuitive trend: for the highest-performing models (MetaCLIP 2 and\nOpenCLIP), the simplest, most basic prompts consistently achieve the best\nresults. Adding descriptive detail significantly degrades performance for\ninstance, MetaCLIP 2's multi-class accuracy drops from 68.8\\% to 55.1\\% a\nphenomenon we term \"prompt overfitting\". Conversely, the lower-performing\nSigLip model shows improved classification on ambiguous classes when given more\ndescriptive, body-cue-based prompts.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13368", "abs": "https://arxiv.org/abs/2510.13368", "authors": ["Yue Xing", "Yingnan Deng", "Heyao Liu", "Ming Wang", "Yun Zi", "Xiaoxuan Sun"], "title": "Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services", "comment": null, "summary": "This paper addresses the challenges of complex dependencies and diverse\nanomaly patterns in cloud service environments by proposing a dependency\nmodeling and anomaly detection method that integrates contrastive learning. The\nmethod abstracts service interactions into a dependency graph, extracts\ntemporal and structural features through embedding functions, and employs a\ngraph convolution mechanism to aggregate neighborhood information for\ncontext-aware service representations. A contrastive learning framework is then\nintroduced, constructing positive and negative sample pairs to enhance the\nseparability of normal and abnormal patterns in the representation space.\nFurthermore, a temporal consistency constraint is designed to maintain\nrepresentation stability across time steps and reduce the impact of short-term\nfluctuations and noise. The overall optimization combines contrastive loss and\ntemporal consistency loss to ensure stable and reliable detection across\nmulti-dimensional features. Experiments on public datasets systematically\nevaluate the method from hyperparameter, environmental, and data sensitivity\nperspectives. Results show that the proposed approach significantly outperforms\nexisting methods on key metrics such as Precision, Recall, F1-Score, and AUC,\nwhile maintaining robustness under conditions of sparse labeling, monitoring\nnoise, and traffic fluctuations. This study verifies the effectiveness of\nintegrating dependency modeling with contrastive learning, provides a complete\ntechnical solution for cloud service anomaly detection, and demonstrates strong\nadaptability and stability in complex environments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13394", "abs": "https://arxiv.org/abs/2510.13394", "authors": ["Xinmiao Huang", "Qisong He", "Zhenglin Huang", "Boxuan Wang", "Zhuoyun Li", "Guangliang Cheng", "Yi Dong", "Xiaowei Huang"], "title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models", "comment": null, "summary": "Spatial reasoning ability is crucial for Vision Language Models (VLMs) to\nsupport real-world applications in diverse domains including robotics,\naugmented reality, and autonomous navigation. Unfortunately, existing\nbenchmarks are inadequate in assessing spatial reasoning ability, especially\nthe \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of\nhuman spatial cognition. In this paper, we propose a unified benchmark,\n\\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that\ncategorizes tasks into four fundamental quadrants:\n\\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic,\n\\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,\nto address the issue of data scarcity, we develop a scalable and automated\npipeline to generate diverse and verifiable spatial reasoning questions,\nresulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE\nBench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA\npairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals\nthat, current VLMs have a large and consistent gap to human competence,\nespecially on multi-step multi-view spatial reasoning. Spatial-DISE offers a\nrobust framework, valuable dataset, and clear direction for future research\ntoward human-like spatial intelligence. Benchmark, dataset, and code will be\npublicly released.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13418", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13418", "abs": "https://arxiv.org/abs/2510.13418", "authors": ["Yifu Luo", "Xinhao Hu", "Keyu Fan", "Haoyuan Sun", "Zeyu Chen", "Bo Xia", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "title": "Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation", "comment": null, "summary": "Reinforcement learning (RL) has garnered increasing attention in\ntext-to-image (T2I) generation. However, most existing RL approaches are\ntailored to either diffusion models or autoregressive models, overlooking an\nimportant alternative: masked generative models. In this work, we propose\nMask-GRPO, the first method to incorporate Group Relative Policy Optimization\n(GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine\nthe transition probability, which is different from current approaches, and\nformulate the unmasking process as a multi-step decision-making problem. To\nfurther enhance our method, we explore several useful strategies, including\nremoving the KL constraint, applying the reduction strategy, and filtering out\nlow-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with\nsubstantial improvements on standard T2I benchmarks and preference alignment,\noutperforming existing state-of-the-art approaches. The code is available on\nhttps://github.com/xingzhejun/Mask-GRPO", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13419", "abs": "https://arxiv.org/abs/2510.13419", "authors": ["Jianhui Zhang", "Sheng Cheng", "Qirui Sun", "Jia Liu", "Wang Luyang", "Chaoyu Feng", "Chen Fang", "Lei Lei", "Jue Wang", "Shuaicheng Liu"], "title": "Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter", "comment": null, "summary": "In this work, we present Patch-Adapter, an effective framework for\nhigh-resolution text-guided image inpainting. Unlike existing methods limited\nto lower resolutions, our approach achieves 4K+ resolution while maintaining\nprecise content consistency and prompt alignment, two critical challenges in\nimage inpainting that intensify with increasing resolution and texture\ncomplexity. Patch-Adapter leverages a two-stage adapter architecture to scale\nthe diffusion model's resolution from 1K to 4K+ without requiring structural\noverhauls: (1) Dual Context Adapter learns coherence between masked and\nunmasked regions at reduced resolutions to establish global structural\nconsistency; and (2) Reference Patch Adapter implements a patch-level attention\nmechanism for full-resolution inpainting, preserving local detail fidelity\nthrough adaptive feature fusion. This dual-stage architecture uniquely\naddresses the scalability gap in high-resolution inpainting by decoupling\nglobal semantics from localized refinement. Experiments demonstrate that\nPatch-Adapter not only resolves artifacts common in large-scale inpainting but\nalso achieves state-of-the-art performance on the OpenImages and\nPhoto-Concept-Bucket datasets, outperforming existing methods in both\nperceptual quality and text-prompt adherence.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13669", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13669", "abs": "https://arxiv.org/abs/2510.13669", "authors": ["Zian Li", "Muhan Zhang"], "title": "CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas", "comment": null, "summary": "Masked autoregressive models (MAR) have recently emerged as a powerful\nparadigm for image and video generation, combining the flexibility of masked\nmodeling with the potential of continuous tokenizer. However, video MAR models\nsuffer from two major limitations: the slow-start problem, caused by the lack\nof a structured global prior at early sampling stages, and error accumulation\nacross the autoregression in both spatial and temporal dimensions. In this\nwork, we propose CanvasMAR, a novel video MAR model that mitigates these issues\nby introducing a canvas mechanism--a blurred, global prediction of the next\nframe, used as the starting point for masked generation. The canvas provides\nglobal structure early in sampling, enabling faster and more coherent frame\nsynthesis. Furthermore, we introduce compositional classifier-free guidance\nthat jointly enlarges spatial (canvas) and temporal conditioning, and employ\nnoise-based canvas augmentation to enhance robustness. Experiments on the BAIR\nand Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality\nvideos with fewer autoregressive steps. Our approach achieves remarkable\nperformance among autoregressive models on Kinetics-600 dataset and rivals\ndiffusion-based methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13704", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13704", "abs": "https://arxiv.org/abs/2510.13704", "authors": ["Johan Obando-Ceron", "Walter Mayor", "Samuel Lavoie", "Scott Fujimoto", "Aaron Courville", "Pablo Samuel Castro"], "title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents", "comment": null, "summary": "Recent works have proposed accelerating the wall-clock training time of\nactor-critic methods via the use of large-scale environment parallelization;\nunfortunately, these can sometimes still require large number of environment\ninteractions to achieve a desired level of performance. Noting that\nwell-structured representations can improve the generalization and sample\nefficiency of deep reinforcement learning (RL) agents, we propose the use of\nsimplicial embeddings: lightweight representation layers that constrain\nembeddings to simplicial structures. This geometric inductive bias results in\nsparse and discrete features that stabilize critic bootstrapping and strengthen\npolicy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial\nembeddings consistently improve sample efficiency and final performance across\na variety of continuous- and discrete-control environments, without any loss in\nruntime speed.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13756", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13756", "abs": "https://arxiv.org/abs/2510.13756", "authors": ["Junhong Shen", "Mu Cai", "Bo Hu", "Ameet Talwalkar", "David A Ross", "Cordelia Schmid", "Alireza Fathi"], "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for\nstructured visuals like charts and diagrams, as pixel-based perception lacks a\nmechanism for verification. To address this, we propose to leverage derendering\n-- the process of reverse-engineering visuals into executable code -- as a new\nmodality for verifiable visual reasoning. Specifically, we propose RECODE, an\nagentic framework that first generates multiple candidate programs to reproduce\nthe input image. It then uses a critic to select the most faithful\nreconstruction and iteratively refines the code. This process not only\ntransforms an ambiguous perceptual task into a verifiable, symbolic problem,\nbut also enables precise calculations and logical inferences later on. On\nvarious visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,\nRECODE significantly outperforms methods that do not leverage code or only use\ncode for drawing auxiliary lines or cropping. Our work demonstrates that\ngrounding visual perception in executable code provides a new path toward more\naccurate and verifiable multimodal reasoning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13542", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13542", "abs": "https://arxiv.org/abs/2510.13542", "authors": ["Martin Licht", "Sara Ketabi", "Farzad Khalvati"], "title": "ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling", "comment": null, "summary": "Topic modeling is a useful tool for analyzing large corpora of written\ndocuments, particularly academic papers. Despite a wide variety of proposed\ntopic modeling techniques, these techniques do not perform well when applied to\nmedical texts. This can be due to the low number of documents available for\nsome topics in the healthcare domain. In this paper, we propose ProtoTopic, a\nprototypical network-based topic model used for topic generation for a set of\nmedical paper abstracts. Prototypical networks are efficient, explainable\nmodels that make predictions by computing distances between input datapoints\nand a set of prototype representations, making them particularly effective in\nlow-data or few-shot learning scenarios. With ProtoTopic, we demonstrate\nimproved topic coherence and diversity compared to two topic modeling baselines\nused in the literature, demonstrating the ability of our model to generate\nmedically relevant topics even with limited data.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13567", "abs": "https://arxiv.org/abs/2510.13567", "authors": ["Omayma Moussadek", "Riccardo Salami", "Simone Calderara"], "title": "DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning", "comment": null, "summary": "Federated continual learning (FCL) enables models to learn new tasks across\nmultiple distributed clients, protecting privacy and without forgetting\npreviously acquired knowledge. However, current methods face challenges\nbalancing performance, privacy preservation, and communication efficiency. We\nintroduce a Distributed Online LoRA for Federated INcremental learning method\nDOLFIN, a novel approach combining Vision Transformers with low-rank adapters\ndesigned to efficiently and stably learn new tasks in federated environments.\nOur method leverages LoRA for minimal communication overhead and incorporates\nDualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on\nCIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet\nheterogeneity settings, DOLFIN consistently surpasses six strong baselines in\nfinal average accuracy while matching their memory footprint. Orthogonal\nlow-rank adapters offer an effective and scalable solution for\nprivacy-preserving continual learning in federated settings.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13649", "abs": "https://arxiv.org/abs/2510.13649", "authors": ["Sanchar Palit", "Subhasis Chaudhuri", "Biplab Banerjee"], "title": "Local-Global Context-Aware and Structure-Preserving Image Super-Resolution", "comment": "10 pages, 11 figures", "summary": "Diffusion models have recently achieved significant success in various image\nmanipulation tasks, including image super-resolution and perceptual quality\nenhancement. Pretrained text-to-image models, such as Stable Diffusion, have\nexhibited strong capabilities in synthesizing realistic image content, which\nmakes them particularly attractive for addressing super-resolution tasks. While\nsome existing approaches leverage these models to achieve state-of-the-art\nresults, they often struggle when applied to diverse and highly degraded\nimages, leading to noise amplification or incorrect content generation. To\naddress these limitations, we propose a contextually precise image\nsuper-resolution framework that effectively maintains both local and global\npixel relationships through Local-Global Context-Aware Attention, enabling the\ngeneration of high-quality images. Furthermore, we propose a distribution- and\nperceptual-aligned conditioning mechanism in the pixel space to enhance\nperceptual fidelity. This mechanism captures fine-grained pixel-level\nrepresentations while progressively preserving and refining structural\ninformation, transitioning from local content details to the global structural\ncomposition. During inference, our method generates high-quality images that\nare structurally consistent with the original content, mitigating artifacts and\nensuring realistic detail restoration. Extensive experiments on multiple\nsuper-resolution benchmarks demonstrate the effectiveness of our approach in\nproducing high-fidelity, perceptually accurate reconstructions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13660", "abs": "https://arxiv.org/abs/2510.13660", "authors": ["Hongyu Qu", "Jianan Wei", "Xiangbo Shu", "Yazhou Yao", "Wenguan Wang", "Jinhui Tang"], "title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild", "comment": "Accepted to NeurIPS 2025; Project page:\n  \\url{https://github.com/quhongyu/OmniGaze}", "summary": "Current 3D gaze estimation methods struggle to generalize across diverse data\ndomains, primarily due to i) the scarcity of annotated datasets, and ii) the\ninsufficient diversity of labeled data. In this work, we present OmniGaze, a\nsemi-supervised framework for 3D gaze estimation, which utilizes large-scale\nunlabeled data collected from diverse and unconstrained real-world environments\nto mitigate domain bias and generalize gaze estimation in the wild. First, we\nbuild a diverse collection of unlabeled facial images, varying in facial\nappearances, background environments, illumination conditions, head poses, and\neye occlusions. In order to leverage unlabeled data spanning a broader\ndistribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a\nreward model to assess the reliability of pseudo labels. Beyond pseudo labels\nas 3D direction vectors, the reward model also incorporates visual embeddings\nextracted by an off-the-shelf visual encoder and semantic cues from gaze\nperspective generated by prompting a Multimodal Large Language Model to compute\nconfidence scores. Then, these scores are utilized to select high-quality\npseudo labels and weight them for loss computation. Extensive experiments\ndemonstrate that OmniGaze achieves state-of-the-art performance on five\ndatasets under both in-domain and cross-domain settings. Furthermore, we also\nevaluate the efficacy of OmniGaze as a scalable data engine for gaze\nestimation, which exhibits robust zero-shot generalization on four unseen\ndatasets.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13656", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13656", "abs": "https://arxiv.org/abs/2510.13656", "authors": ["Priyobrata Mondal", "Faizanuddin Ansari", "Swagatam Das"], "title": "Rebalancing with Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced Classification", "comment": null, "summary": "The class imbalance problem refers to the insufficiency of data in certain\nclasses, which causes a classifier to be biased toward the majority class.\nDistribution calibration is a technique that seeks to estimate a more accurate\nclass distribution based on an observed or estimated one. To address this\nissue, we propose a distribution calibration-based method-Rebalancing with\nCalibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced\nClassification, which estimates the distribution parameters of the minority\nclasses using weighted parameters derived from a mixture of Gaussian components\nfrom both the majority and intermediate classes. An encoder-decoder network is\ntrained to preserve the structure of the imbalanced data and prevent\ndisentanglement. After training, feature vectors extracted from the encoder are\nused to generate synthetic samples through our distribution calibration\nstrategy. This approach effectively mitigates the overgeneralization problem\nthat arises when only the distribution of the majority class is used to\napproximate the minority class statistics. Instead, our method calibrates the\nparameters by leveraging the distribution of data points in neighboring\nregions. Experimental results demonstrate that the proposed method achieves\nsuperior classification performance compared to several baseline and\nstate-of-the-art techniques across a diverse range of image, text, and tabular\ndatasets.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13720", "abs": "https://arxiv.org/abs/2510.13720", "authors": ["Fabio Musio", "Norman Juchler", "Kaiyuan Yang", "Suprosanna Shit", "Chinmay Prabhakar", "Bjoern Menze", "Sven Hirsch"], "title": "Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm", "comment": null, "summary": "The Circle of Willis (CoW) is a critical network of arteries in the brain,\noften implicated in cerebrovascular pathologies. Voxel-level segmentation is an\nimportant first step toward an automated CoW assessment, but a full\nquantitative analysis requires centerline representations. However,\nconventional skeletonization techniques often struggle to extract reliable\ncenterlines due to the CoW's complex geometry, and publicly available\ncenterline datasets remain scarce. To address these challenges, we used a\nthinning-based skeletonization algorithm to extract and curate centerline\ngraphs and morphometric features from the TopCoW dataset, which includes 200\nstroke patients, each imaged with MRA and CTA. The curated graphs were used to\ndevelop a baseline algorithm for centerline and feature extraction, combining\nU-Net-based skeletonization with A* graph connection. Performance was evaluated\non a held-out test set, focusing on anatomical accuracy and feature robustness.\nFurther, we used the extracted features to predict the frequency of fetal PCA\nvariants, confirm theoretical bifurcation optimality relations, and detect\nsubtle modality differences. The baseline algorithm consistently reconstructed\ngraph topology with high accuracy (F1 = 1), and the average Euclidean node\ndistance between reference and predicted graphs was below one voxel. Features\nsuch as segment radius, length, and bifurcation ratios showed strong\nrobustness, with median relative errors below 5% and Pearson correlations above\n0.95. Our results demonstrate the utility of learning-based skeletonization\ncombined with graph connection for anatomically plausible centerline\nextraction. We emphasize the importance of going beyond simple voxel-based\nmeasures by evaluating anatomical accuracy and feature robustness. The dataset\nand baseline algorithm have been released to support further method development\nand clinical research.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13745", "abs": "https://arxiv.org/abs/2510.13745", "authors": ["Tianshuo Xu", "Kai Wang", "Zhifei Chen", "Leyi Wu", "Tianshui Wen", "Fei Chao", "Ying-Cong Chen"], "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy", "comment": "22 pages", "summary": "Computational replication of Chinese calligraphy remains challenging.\nExisting methods falter, either creating high-quality isolated characters while\nignoring page-level aesthetics like ligatures and spacing, or attempting page\nsynthesis at the expense of calligraphic correctness. We introduce\n\\textbf{UniCalli}, a unified diffusion framework for column-level recognition\nand generation. Training both tasks jointly is deliberate: recognition\nconstrains the generator to preserve character structure, while generation\nprovides style and layout priors. This synergy fosters concept-level\nabstractions that improve both tasks, especially in limited-data regimes. We\ncurated a dataset of over 8,000 digitized pieces, with ~4,000 densely\nannotated. UniCalli employs asymmetric noising and a rasterized box map for\nspatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The\nmodel achieves state-of-the-art generative quality with superior ligature\ncontinuity and layout fidelity, alongside stronger recognition. The framework\nsuccessfully extends to other ancient scripts, including Oracle bone\ninscriptions and Egyptian hieroglyphs. Code and data can be viewed in\n\\href{https://github.com/EnVision-Research/UniCalli}{this URL}.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.13809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13809", "abs": "https://arxiv.org/abs/2510.13809", "authors": ["Sihui Ji", "Xi Chen", "Xin Tao", "Pengfei Wan", "Hengshuang Zhao"], "title": "PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning", "comment": "Project Page: https://sihuiji.github.io/PhysMaster-Page/", "summary": "Video generation models nowadays are capable of generating visually realistic\nvideos, but often fail to adhere to physical laws, limiting their ability to\ngenerate physically plausible videos and serve as ''world models''. To address\nthis issue, we propose PhysMaster, which captures physical knowledge as a\nrepresentation for guiding video generation models to enhance their\nphysics-awareness. Specifically, PhysMaster is based on the image-to-video task\nwhere the model is expected to predict physically plausible dynamics from the\ninput image. Since the input image provides physical priors like relative\npositions and potential interactions of objects in the scenario, we devise\nPhysEncoder to encode physical information from it as an extra condition to\ninject physical knowledge into the video generation process. The lack of proper\nsupervision on the model's physical performance beyond mere appearance\nmotivates PhysEncoder to apply reinforcement learning with human feedback to\nphysical representation learning, which leverages feedback from generation\nmodels to optimize physical representations with Direct Preference Optimization\n(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for\nimproving physics-awareness of PhysEncoder and thus of video generation,\nproving its ability on a simple proxy task and generalizability to wide-ranging\nphysical scenarios. This implies that our PhysMaster, which unifies solutions\nfor various physical processes via representation learning in the reinforcement\nlearning paradigm, can act as a generic and plug-in solution for physics-aware\nvideo generation and broader applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
